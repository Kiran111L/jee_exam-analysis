# -*- coding: utf-8 -*-
"""Mains_exam-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qWXBY4OcsL_ra8QUvLRA72k58ZjCHUZT
"""

pip install pandas xlrd scikit-learn

# --- Step 1: Import necessary libraries ---
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# --- Step 2: Load your Excel dataset with the correct file path ---
file_path = '/content/Mains_exam.xls'

try:
    # We now specify 'header=2' to tell pandas to use the third row (index 2) as column headers.
    # We also specify 'engine=xlrd' for .xls files.
    df = pd.read_excel(file_path, header=2, engine='xlrd')
    print("Dataset loaded successfully!")
    print("----------------------------\n")
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")
    print("Please make sure the file exists at the specified path.")
    exit()
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    print("Please ensure you have installed the necessary libraries like 'xlrd'.")
    exit()

# --- Step 3: Data Preparation for Machine Learning ---
# We will drop the numerical topic codes and other text columns that are not needed as features.
df_prepared = df.drop(columns=['Topic_Code', 'Subtopic_Code', 'Sample_Problem_Style'])

# We will now convert your text-based columns into a numerical format for the model.
# This is a crucial step for machine learning algorithms.
df_encoded = pd.get_dummies(df_prepared, columns=[
    'Subject', 'Topic', 'Subtopic', 'Difficulty',
    'Question_Type', 'Subtopic_Combo', 'Question_Tone'
], drop_first=True)

print("Columns after encoding (now all numerical):")
print(df_encoded.columns)

# --- Step 4: Define Features (X) and Target (y) ---
# Features (X) are the columns we use to predict with.
# The Target (y) is the column we want to predict ('Count').
X = df_encoded.drop(columns=['Count', 'Year'])
y = df_encoded['Count']

# --- Step 5: Split the data into Training and Testing sets ---
# We train the model on 80% of the data and test its accuracy on the remaining 20%.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTraining on {len(X_train)} rows and testing on {len(X_test)} rows.")

# --- Step 6: Choose and Train the Machine Learning Model ---
# We use a Random Forest Regressor for this prediction task.
print("\nTraining the Random Forest Regressor model...")
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# --- Step 7: Make Predictions and Evaluate the Model ---
print("Making predictions on the test data...")
predictions = model.predict(X_test)

# The predictions will be numbers, which can be floating-point numbers.
# We can round them to get a more sensible question count.
rounded_predictions = [round(p) for p in predictions]

print("\n--- Model Results ---")
print("Actual Counts:", list(y_test))
print("Predicted Counts:", rounded_predictions)

# Evaluate the model's performance
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f"\nMean Absolute Error (MAE): {mae:.2f}")
print(f"R-squared (R²): {r2:.2f}")

# MAE tells you the average difference between the predicted and actual count.
# R² tells you how well your model explains the variation in the data (1.0 is a perfect score).

# --- Step 1: Import necessary libraries ---
import pandas as pd
import matplotlib.pyplot as plt

# --- Step 2: Load your Excel dataset with the correct file path ---
file_path = '/content/Mains_exam.xls'

try:
    df = pd.read_excel(file_path, header=2, engine='xlrd')
    print("Dataset loaded successfully!")
    print("----------------------------\n")
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")
    print("Please make sure the file exists at the specified path.")
    exit()

# --- Step 3: Get user input for the subject ---
while True:
    user_subject = input("Enter a subject (Maths or Physics): ").strip().capitalize()
    if user_subject in ['Maths', 'Physics']:
        break
    else:
        print("Invalid input. Please enter 'Maths' or 'Physics'.")

# --- Step 4: Filter data based on user input ---
df_filtered = df[df['Subject'] == user_subject]

if df_filtered.empty:
    print(f"No data found for the subject: {user_subject}")
    exit()

print(f"\nGenerating charts for {user_subject}...")

# --- Chart 1: Question Count per Topic (Bar Chart) ---
plt.figure(figsize=(14, 8))
df_topic_counts = df_filtered.groupby('Topic')['Count'].sum().sort_values(ascending=False)
df_topic_counts.plot(kind='bar', color='skyblue')
plt.title(f'Total Questions per Topic in {user_subject} (2000-2025)')
plt.xlabel('Topic')
plt.ylabel('Total Question Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(f'{user_subject}_questions_by_topic.png')
plt.close()
print(f"Chart saved as {user_subject}_questions_by_topic.png")

# --- Chart 2: Question Distribution by Difficulty (Pie Chart) ---
plt.figure(figsize=(8, 8))
df_difficulty_counts = df_filtered.groupby('Difficulty')['Count'].sum()
df_difficulty_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['gold', 'lightcoral', 'lightskyblue'])
plt.title(f'Difficulty Distribution in {user_subject}')
plt.ylabel('')
plt.tight_layout()
plt.savefig(f'{user_subject}_difficulty_distribution.png')
plt.close()
print(f"Chart saved as {user_subject}_difficulty_distribution.png")

# --- Chart 3: Question Type Distribution (Bar Chart) ---
plt.figure(figsize=(8, 6))
df_type_counts = df_filtered.groupby('Question_Type')['Count'].sum()
df_type_counts.plot(kind='bar', color=['salmon', 'lightgreen'])
plt.title(f'Question Type Distribution in {user_subject}')
plt.xlabel('Question Type')
plt.ylabel('Total Question Count')
plt.xticks(rotation=0)
plt.tight_layout()
plt.savefig(f'{user_subject}_question_type_distribution.png')
plt.close()
print(f"Chart saved as {user_subject}_question_type_distribution.png")